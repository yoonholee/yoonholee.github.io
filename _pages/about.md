---
layout: about
title: About
permalink: /
# description: <a href="#">Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: prof_pic.jpg

news: false
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true
---

I'm a Ph.D. candidate at Stanford CS, advised by [Chelsea Finn](https://ai.stanford.edu/~cbfinn/) and part of the [IRIS lab](https://irislab.stanford.edu/).
I am affiliated with [SAIL](https://ai.stanford.edu/), [CRFM](https://crfm.stanford.edu/), and the [ML Group](http://ml.stanford.edu/) at Stanford.
My research is generously supported through grants and fellowships from [OpenAI](https://openai.com/index/superalignment-fast-grants/) and [KFAS](https://eng.kfas.or.kr/theme/kfaschanel/intl_scholarship_5.php).
During my mandatory military service in South Korea, I served as a research scientist at [Kakao](https://www.kakaocorp.com/) and [AITRICS](https://www.aitrics.com/), collaborating with [Juho Lee](https://juho-lee.github.io/).
I hold a master's degree in Computer Science (advised by [Seungjin Choi](http://mlg.postech.ac.kr/~seungjin)) from [POSTECH](https://www.postech.ac.kr/eng/).

Here are some key questions that guide my research:

- **Teaching strong models**: Pre-trained models already hold much of what we need to teach. How can we develop teaching paradigms that better leverage these inherent capabilities?
- **Underspecification**: No dataset fully specifies its intended task. How can we help models recognize and represent the multiple valid interpretations of the data, and how do we best leverage this diversity of hypotheses?
- **Understanding information**: Data carries an underlying essence ("information") that goes beyond its specific form. How can we better conceptualize this notion and understand how models extract, process, and communicate it?

My name (윤호) is pronounced like ‘you-know’ said quickly, with an emphasis on 'you'.
[Here](https://ipa-reader.com/?text=%2Fju%3Ano%CA%8A%2F) is a good approximation.

<div class="selected-papers">
<h3>Selected Papers</h3>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2412.08812">
Test-Time Alignment via Hypothesis Reweighting
</a>
<p class="authors">
Yoonho Lee, Jonathan Williams, Henrik Marklund, Archit Sharma, Eric Mitchell, Anikait Singh, Chelsea Finn
</p>
<p class="venue">
arXiv preprint
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2402.03715">
Clarify: Improving Model Robustness with Natural Language Corrections
</a>
<p class="authors">
Yoonho Lee, Michelle Lam, Helena Vasconcelos, Michael S. Bernstein, Chelsea Finn
</p>
<p class="venue">
UIST 2024, NeurIPS 2023 workshops XAIA and ICBINB
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2401.10220">
AutoFT: Learning an Objective for Robust Fine-Tuning
</a>
<p class="authors">
Caroline Choi*, Yoonho Lee*, Annie S. Chen, Allan Zhou, Aditi Raghunathan, Chelsea Finn
</p>
<p class="venue">
NeurIPS 2023 workshop DistShift
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2302.05441">
Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features
</a>
<p class="authors">
Annie S. Chen*, Yoonho Lee*, Amrith Setlur, Sergey Levine, Chelsea Finn
</p>
<p class="venue">
ICLR 2024 (spotlight)
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2301.11305">
DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature
</a>
<p class="authors">
Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn
</p>
<p class="venue">
ICML 2023 (long oral)
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2210.11466">
Surgical Fine-Tuning Improves Adaptation to Distribution Shifts
</a>
<p class="authors">
Yoonho Lee*, Annie S. Chen*, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, Chelsea Finn
</p>
<p class="venue">
ICLR 2023
</p>
</div>

<div class="paper">
<a main-paper-link href="https://arxiv.org/abs/2202.03418">
Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement
</a>
<p class="authors">
Yoonho Lee, Huaxiu Yao, Chelsea Finn
</p>
<p class="venue">
ICLR 2023
</p>
</div>

<div class="paper">
<a main-paper-link href="https://proceedings.mlr.press/v97/lee19d/lee19d.pdf">
Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks
</a>
<p class="authors">
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, Yee Whye Teh
</p>
<p class="venue">
ICML 2019
</p>
</div>

</div>
