%-------------------------------------------------------------------------------
%	SECTION TITLE
%-------------------------------------------------------------------------------
\cvsection{Publications}

\newcommand{\me}{\underline{Yoonho Lee}}
\newcommand{\meeq}{\underline{Yoonho Lee*}}
\newcommand{\cf}{Chelsea Finn}
\newcommand{\ac}{Annie S. Chen}

\newcommand{\ljh}{Juho Lee}
\newcommand{\sjh}{Sung Ju Hwang}
\newcommand{\ehy}{Eunho Yang}
\newcommand{\sjc}{Seungjin Choi}
\newcommand{\jtk}{Jungtaek Kim}
\newcommand{\ywt}{Yee Whye Teh}
%-------------------------------------------------------------------------------
%	CONTENT
%-------------------------------------------------------------------------------
\autopubcount % Automatically count publications from previous compilation

\vspace{0pt}
\cvpub
{\me, Joseph Boen, \cf}
{\href{https://arxiv.org/abs/2511.07919}{Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison}}
{\textbf{arXiv:2511.07919} \textcolor{gray}{\textit{(preprint)}}}

\cvpub
{Yuxiao Qu*, Anikait Singh*, \meeq, Amrith Setlur, Ruslan Salakhutdinov, \cf, Aviral Kumar}
{\href{https://arxiv.org/abs/2510.02263}{RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems}}
{\textbf{ICML 2025} (42\textsuperscript{nd} International Conference on Machine Learning) Workshops: AI for Math, PRAL, ES-FoMo}

\cvpub
{\me, Jonathan Williams, Henrik Marklund, Archit Sharma, Eric Mitchell, Anikait Singh, \cf}
{\href{https://arxiv.org/abs/2412.08812}{Test-Time Alignment via Hypothesis Reweighting}}
{\textbf{ICML 2025} (42\textsuperscript{nd} International Conference on Machine Learning) Workshop PUT}

\cvpub
{Yuejiang Liu, Jubayer Ibn Hamid, Annie Xie, \me, Maximilian Du, \cf}
{\href{https://arxiv.org/abs/2408.17355}{Bidirectional Decoding: Improving Action Chunking via Closed-Loop Resampling}}
{\textbf{ICLR 2025} (13\textsuperscript{th} International Conference on Learning Representations)}

\cvpub
{\me, Michelle Lam, Helena Vasconcelos, Michael S. Bernstein, Chelsea Finn}
{\href{https://arxiv.org/abs/2402.03715}{Clarify: Improving Model Robustness With Natural Language Corrections}}
{\textbf{UIST 2024} (ACM Symposium on User Interface Software and Technology), \textbf{NeurIPS 2023} (37\textsuperscript{th} Conference on Neural Information Processing Systems) Workshops: XAIA, ICBINB}

\cvpub
{Caroline Choi*, \meeq, Annie S. Chen, Allan Zhou, Aditi Raghunathan, Chelsea Finn}
{\href{https://arxiv.org/abs/2401.10220}{AutoFT: Learning an Objective for Robust Fine-Tuning}}
{Workshop on Distribution Shifts, \textbf{NeurIPS 2023} (37\textsuperscript{th} Conference on Neural Information Processing Systems)}

\cvpub
{Annie S. Chen, \me, Amrith Setlur, Sergey Levine, Chelsea Finn}
{\href{https://arxiv.org/abs/2306.11120}{Confidence-Based Model Selection: When to Take Shortcuts for Subpopulation Shifts}}
{Workshop on Distribution Shifts, \textbf{NeurIPS 2023} (37\textsuperscript{th} Conference on Neural Information Processing Systems)}

\cvpub
{Caroline Choi*, Fahim Tajwar*, \meeq, Huaxiu Yao, Ananya Kumar, Chelsea Finn}
{\href{https://arxiv.org/abs/2306.04974}{Conservative Prediction via Data-Driven Confidence Minimization}}
{Transactions on Machine Learning Research (\textbf{TMLR 2024}), \textbf{ICLR 2023} (11\textsuperscript{th} International Conference on Learning Representations) Workshops: TrustML, ME-FoMo}

\cvpub
{Annie S. Chen*, \meeq, Amrith Setlur, Sergey Levine, Chelsea Finn}
{\href{https://arxiv.org/abs/2302.05441}{Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features}}
{\textbf{ICLR 2024, Spotlight} (12\textsuperscript{th} International Conference on Learning Representations, top 5\% of submissions), \textbf{ICLR 2023} (11\textsuperscript{th} International Conference on Learning Representations) Workshops: TrustML (Oral), ME-FoMo}

\cvpub
{Johnathan Wenjia Xie, \me, Annie S. Chen, Chelsea Finn}
{\href{https://openreview.net/forum?id=HiYMiZYwkw}{Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning}}
{\textbf{ICLR 2024} (12\textsuperscript{th} International Conference on Learning Representations)}

\cvpub
{Eric Mitchell, \me, Alexander Khazatsky, Christopher D Manning, \cf}
{\href{https://arxiv.org/abs/2301.11305}{DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature}}
{\textbf{ICML 2023, Oral} (40\textsuperscript{th} International Conference on Machine Learning, top 2\% of submissions)}

\cvpub
{\meeq, \ac*, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, \cf}
{\href{https://arxiv.org/abs/2210.11466}{Surgical Fine-Tuning Improves Adaptation to Distribution Shifts}}
{\textbf{ICLR 2023} (11\textsuperscript{th} International Conference on Learning Representations)}

\cvpub
{\me, Huaxiu Yao, \cf}
{\href{https://arxiv.org/abs/2202.03418}{Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement}}
{\textbf{ICLR 2023} (11\textsuperscript{th} International Conference on Learning Representations)}

\cvpub
{\me, \cf, Stefano Ermon}
{\href{https://openreview.net/forum?id=I1bW14EvUF7}{Relaxing the Kolmogorov Structure Function for Realistic Computational Constraints}}
{Workshop on Information-Theoretic Principles in Cognitive Systems, \textbf{NeurIPS 2022} (36\textsuperscript{th} Conference on Neural Information Processing Systems)}

\cvpub
{Balhae Kim, Jungwon Choi, Seanie Lee, \me, Jung-Woo Ha, \ljh}
{\href{https://arxiv.org/abs/2210.06205}{On Divergence Measures for Bayesian Pseudocoresets}}
{\textbf{NeurIPS 2022} (36\textsuperscript{th} Conference on Neural Information Processing Systems)}

\cvpub
{Huaxiu Yao*, Caroline Choi*, Bochuan Cao, \me, Pang Wei Koh, \cf}
{\href{https://arxiv.org/abs/2211.14238}{Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time}}
{\textbf{NeurIPS 2022} (36\textsuperscript{th} Conference on Neural Information Processing Systems), Datasets \& Benchmarks track}

\cvpub
{\me, Wonjae Kim, Wonpyo Park, \sjc}
{ \href{https://arxiv.org/abs/1905.11656}
  {Discrete Infomax Codes for Supervised Representation Learning}}
{Special issue "Theory and Applications of Information Processing Algorithms", \textbf{Entropy 2022}}

\cvpub
{Giung Nam*, Jongmin Yoon*, \me, \ljh}
{\href{https://arxiv.org/abs/2110.14149}{Diversity Matters When Learning From Ensembles}}
{\textbf{NeurIPS 2021} (35\textsuperscript{th} Conference on Neural Information Processing Systems)}

\cvpub
{Minkyo Seo*, \meeq, Suha Kwak}
{\href{https://arxiv.org/abs/2107.01900}{On the Distribution of Penultimate Activations of Classification Networks}}
{\textbf{UAI 2021} (37\textsuperscript{th} Conference on Uncertainty in Artificial Intelligence)}

\cvpub
{\me, \ljh, \sjh, \ehy, \sjc}
{\href{https://arxiv.org/abs/2008.02953}
  {Neural Complexity Measures}}
{\textbf{NeurIPS 2020} (34\textsuperscript{th} Conference on Neural Information Processing Systems)}

\cvpub
{Juho Lee*, \meeq, \jtk, \ehy, \sjh, \ywt}
{\href{https://arxiv.org/abs/2008.02956}
  {Bootstrapping Neural Processes}}
{\textbf{NeurIPS 2020} (34\textsuperscript{th} Conference on Neural Information Processing Systems)}

\cvpub
{Wonjae Kim, \me}
{\href{https://papers.nips.cc/paper/8835-learning-dynamics-of-attention-human-prior-for-interpretable-machine-reasoning.pdf}
  {Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning}}
{\textbf{NeurIPS 2019} (33\textsuperscript{rd} Conference on Neural Information Processing Systems)}

\cvpub
{\ljh, \me, \ywt}
{\href{https://arxiv.org/abs/1909.13433}
  {Deep Amortized Clustering} }
{Sets and Partitions Workshop at \textbf{NeurIPS 2019} (33\textsuperscript{rd} Conference on Neural Information Processing Systems, oral presentation)}

\cvpub
{\ljh, \me, \jtk, Adam Kosiorek, \sjc, \ywt}
{\href{http://proceedings.mlr.press/v97/lee19d/lee19d.pdf}
  {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks}}
{\textbf{ICML 2019} (36\textsuperscript{th} International Conference on Machine Learning)}

\cvpub
{\me \hspace{1pt}, \sjc}
{\href{http://proceedings.mlr.press/v80/lee18a/lee18a.pdf}
  {Gradient-based meta-learning with learned layerwise metric and subspace}}
{\textbf{ICML 2018} (35\textsuperscript{th} International Conference on Machine Learning)}
