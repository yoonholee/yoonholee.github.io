<!DOCTYPE html>
<html lang="en">

<!-- Head -->

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Metadata, OpenGraph and Schema.org -->




<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<link rel="canonical" href="https://www.yoonholee.com/papers/">
<meta name="robots" content="index, follow">




<title>Papers | Yoonho Lee</title>


<meta name="author" content="Yoonho  Lee">
<meta name="description" content="Personal website of Yoonho Lee, Ph.D. candidate at Stanford CS advised by Chelsea Finn, researching machine learning with a focus on teaching strong models, underspecification, and understanding information. Affiliated with Stanford's SAIL, CRFM, and ML Group.">
<meta name="keywords" content="Yoonho Lee, Stanford, CS, PhD, Machine Learning, DetectGPT, DivDis, Set Transformer, Robustness, Underspecification, South Korea, Chelsea Finn">












<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Performance optimizations -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>

<!-- Bootstrap & MDB -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

<!-- Styles -->
 <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%BB%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://www.yoonholee.com/papers/">

    <!-- Dark Mode -->
    
</head>

<!-- Body -->

<body class="fixed-top-nav sticky-bottom-footer">

  <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/">Yoonho Lee</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">About</a>
          </li>

          <!-- Blog navigation removed - blog accessible via permalink only -->

          <!-- Other pages -->
          <li class="nav-item active">
            <a class="nav-link" href="/papers/">Papers<span class="sr-only">(current)</span></a>
          </li>
<li class="nav-item">
            <a class="nav-link" href="/assets/pdf/CV.pdf">CV </a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>


  <!-- Content -->
  <div class="container mt-5">
    <!-- page.html -->
<div class="post">
  <article>
    <div class="publications">


  <div class="year">2025</div>
  <ol class="bibliography">
<li>
<div class="row publication-row" id="qu2024rlad">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICML-W</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems</b></span>

    <!-- Author -->
    <div class="author">
      

      Yuxiao Qu*, Anikait Singh*, <strong>Yoonho Lee*</strong>, Amrith Setlur, Ruslan Salakhutdinov, Chelsea Finn, Aviral Kumar</div>

    <span class="periodical">
      
      <em>ICML 2025 workshops: AI for Math, PRAL, ES-FoMo</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2510.02263" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: </strong> </p>
      <p>Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement "algorithmic procedures" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="lee2024hyre">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICML-W</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Test-Time Alignment via Hypothesis Reweighting</b></span>

    <!-- Author -->
    <div class="author">
      

      <strong>Yoonho Lee</strong>, Jonathan Williams, Henrik Marklund, Archit Sharma, Eric Mitchell, Anikait Singh, Chelsea Finn</div>

    <span class="periodical">
      
      <em>ICML 2025 Workshop PUT</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2412.08812" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Adapting models to test-time preferences by reweighting ensemble members.</strong> </p>
      <p>Large pretrained models often struggle with underspecified tasks – situations where the training data does not fully define the desired behavior. For example, chatbots must handle diverse and often conflicting user preferences, requiring adaptability to various user needs. We propose a novel framework to address the general challenge of aligning models to test-time user intent, which is rarely fully specified during training. Our approach involves training an efficient ensemble, i.e., a single neural network with multiple prediction heads, each representing a different function consistent with the training data. Our main contribution is HyRe, a simple adaptation technique that dynamically reweights ensemble members at test time using a small set of labeled examples from the target distribution, which can be labeled in advance or actively queried from a larger unlabeled pool. By leveraging recent advances in scalable ensemble training, our method scales to large pretrained models, with computational costs comparable to fine-tuning a single model. We empirically validate HyRe in several underspecified scenarios, including personalization tasks and settings with distribution shifts. Additionally, with just five preference pairs from each target distribution, the same ensemble adapted via HyRe outperforms the prior state-of-the-art 2B-parameter reward model accuracy across 18 evaluation distributions.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="liu2024bid">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICLR</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Bidirectional Decoding: Improving Action Chunking via Closed-Loop Resampling</b></span>

    <!-- Author -->
    <div class="author">
      

      Yuejiang Liu*, Jubayer Ibn Hamid*, Annie Xie, <strong>Yoonho Lee</strong>, Maximilian Du, Chelsea Finn</div>

    <span class="periodical">
      
      <em>ICLR 2025</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2408.17355" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/YuejiangLIU/bid_diffusion" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Improving robot action chunking by combining temporal consistency with reactivity.</strong> </p>
      <p>
    Predicting and executing a sequence of actions without intermediate replanning, known as action chunking, is increasingly used in robot learning from human demonstrations. Yet, its reported effects on the learned policy are inconsistent: some studies find it crucial for achieving strong results, while others observe decreased performance. In this paper, we first dissect how action chunking impacts the divergence between a learner and a demonstrator. We find that action chunking allows the learner to better capture the temporal dependencies in demonstrations but at the cost of reduced reactivity in stochastic environments. To address this tradeoff, we propose Bidirectional Decoding (BID), a test-time inference algorithm that bridges action chunking with closed-loop operations. BID samples multiple predictions at each time step and searches for the optimal one based on two criteria: (i) backward coherence, which favors samples that align with previous decisions; (ii) forward contrast, which seeks samples of high likelihood for future plans. By coupling decisions within and across action chunks, BID promotes consistency over time while maintaining reactivity to unexpected changes. Experimental results show that BID boosts the performance of two state-of-the-art generative policies across seven simulation benchmarks and two real-world tasks. Code and videos are available at <a href="https://github.com/YuejiangLIU/bid_diffusion" rel="external nofollow noopener" target="_blank">https://github.com/YuejiangLIU/bid_diffusion</a>.
    </p>
    </span>
    
  </div>
</div>
</li>
</ol>

  <div class="year">2024</div>
  <ol class="bibliography">
<li>
<div class="row publication-row" id="lee2024clarify">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">UIST</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Clarify: Improving Model Robustness With Natural Language Corrections</b></span>

    <!-- Author -->
    <div class="author">
      

      <strong>Yoonho Lee</strong>, Michelle Lam, Helena Vasconcelos, Michael S. Bernstein, Chelsea Finn</div>

    <span class="periodical">
      
      <em>UIST 2024<br>NeurIPS 2023 Workshops: XAIA, ICBINB</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2402.03715" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/yoonholee/Clarify" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: A natural language interface for high-level feedback on model misconceptions; works at ImageNet scale.</strong> </p>
      <p>
    The standard way to teach models is by feeding them lots of data. However, this approach often teaches models incorrect ideas because they pick up on misleading signals in the data. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Prior methods incorporate forms of additional instance-level supervision, such as labels for misleading features or additional labels for debiased data. However, such strategies require a large amount of labeler effort. We hypothesize that people are good at providing textual feedback at the level of concepts, a capability which existing frameworks for teaching do not leverage. We propose Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model’s consistent failure patterns. Then, in an entirely automated way, we use such descriptions to improve the training process. Our user studies show that non-expert users can successfully describe model misconceptions via Clarify, leading to increased worst-case performance in two datasets. We additionally conduct a case study on a large-scale image dataset, ImageNet, where we use Clarify to find and rectify 31 novel hard subpopulations.
    </p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="choi2023tcm">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">TMLR</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Conservative Prediction via Data-Driven Confidence Minimization</b></span>

    <!-- Author -->
    <div class="author">
      

      Caroline Choi*, Fahim Tajwar*, <strong>Yoonho Lee*</strong>, Huaxiu Yao, Ananya Kumar, Chelsea Finn</div>

    <span class="periodical">
      
      <em>TMLR 2024<br>ICLR 2023 workshops: TrustML, ME-FoMo</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2306.04974" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/tajwarfahim/dcm" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Conservative prediction by reducing confidence on relevant hard examples.</strong> </p>
      <p>
    Errors of machine learning models can be prohibitively costly, especially in safety-critical settings such as healthcare. However, machine learning may be applicable to such scenarios if the learned model can abstain and defer to a human on difficult examples instead of making errors. In safety-critical settings, we prefer conservative models that defer to humans at the cost of some overall accuracy. Unfortunately, selective classification and out-of-distribution detection are notably difficult as it is hard to anticipate all possible examples. To mitigate this challenge, we focus on the transductive setting, where unlabeled examples from the test distribution are available during training. We propose transductive confidence minimization (TCM), which minimizes prediction confidence on unlabeled test examples while simultaneously optimizing the training objective. We theoretically show that TCM learns a lower bound on the true confidence, and that this property can be leveraged to provably detect examples that are sufficiently different from training examples, regardless of what distribution they came from. In our experiments, TCM consistently shows high performance, achieving the highest OOD detection performance compared to 6 other methods on 9 out of 10 ID-&gt;OOD pairs and consistently outperforming methods for selective classification in settings where we test on data from a previously unseen distribution.
    </p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="choi2024autoft">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS-W</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Robust Fine-Tuning by Learning the Objective with Bi-Level Optimization</b></span>

    <!-- Author -->
    <div class="author">
      

      Caroline Choi*, <strong>Yoonho Lee*</strong>, Annie S. Chen, Allan Zhou, Aditi Raghunathan, Chelsea Finn</div>

    <span class="periodical">
      
      <em>NeurIPS 2023 Workshop on Distribution Shifts</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2401.10220" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/cchoi1/auto-ft" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Robust fine-tuning by optimizing hyperparameters on OOD data; state-of-the-art on WILDS.</strong> </p>
      <p>
    Large pretrained models encode rich representations that should, in principle, facilitate adaptation to downstream tasks by fine-tuning. However, fine-tuning a model on one data distribution often degrades performance on related distributions. Recent works propose hand-crafted fine-tuning algorithms to mitigate this issue, but these may not fully capture the relevant information for the task at hand. Since the optimal fine-tuning procedure depends on the relationship between the pretrained model and the downstream task, a data-driven approach which automatically searches the space of fine-tuning procedures is desirable. We propose LOFT, which optimizes the fine-tuning procedure such that resulting models are more robust to distribution shifts. Concretely, LOFT leverages a small validation set from a distribution that is different from both the training set and the test set. LOFT performs bi-level optimization to search for a loss function for which fine-tuning results in high performance on the validation set. Our experiments on nine naturally occurring distribution shifts show that LOFT outperforms existing robust fine-tuning methods in generalization to unseen distributions. Notably, LOFT achieves a new state-of-the-art on the WILDS iWildCam and FMoW benchmarks, outperforming the previous best methods by 6.0% and 1.5%, respectively.
    </p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="chen2023pro2">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICLR</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features</b></span>

    <!-- Author -->
    <div class="author">
      

      Annie S. Chen*, <strong>Yoonho Lee*</strong>, Amrith Setlur, Sergey Levine, Chelsea Finn</div>

    <span class="periodical">
      
      <em>ICLR 2024 (<b>Spotlight Presentation, top 5%</b>) <br>
    ICLR 2023 workshops: TrustML (<b>Oral</b>), ME-FoMo</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2302.05441" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: A lightweight and sample-efficient approach that learns diverse linear features and adapts to a target distribution by interpolating among them with a small target dataset.</strong> </p>
      <p>
    Conventional approaches to robustness try to learn a model based on causal features. However, identifying maximally robust or causal features may be difficult in some scenarios, and in others, non-causal "shortcut" features may actually be more predictive. We propose a lightweight, sample-efficient approach that learns a diverse set of features and adapts to a target distribution by interpolating these features with a small target dataset. Our approach, Project and Probe (Pro2), first learns a linear projection that maps a pre-trained embedding onto orthogonal directions while being predictive of labels in the source dataset. The goal of this step is to learn a variety of predictive features, so that at least some of them remain useful after distribution shift. Pro2 then learns a linear classifier on top of these projected features using a small target dataset. We theoretically show that Pro2 learns a projection matrix that is optimal for classification in an information-theoretic sense, resulting in better generalization due to a favorable bias-variance tradeoff. Our experiments on four datasets, with multiple distribution shift settings for each, show that Pro2 improves performance by 5-15% when given limited target data compared to prior methods such as standard linear probing.
    </p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="xie2024sma">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICLR</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning</b></span>

    <!-- Author -->
    <div class="author">
      

      Johnathan Wenjia Xie, <strong>Yoonho Lee</strong>, Annie S. Chen, Chelsea Finn</div>

    <span class="periodical">
      
      <em>ICLR 2024</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://openreview.net/forum?id=HiYMiZYwkw" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Self-supervised learning by predicting masked features, applicable to novel data modalities.</strong> </p>
      <p>Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.
</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="xie2024temperature">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">EMNLP</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Calibrating Language Models With Adaptive Temperature Scaling</b></span>

    <!-- Author -->
    <div class="author">
      

      Johnathan Wenjia Xie*, Annie S. Chen*, <strong>Yoonho Lee</strong>, Eric Mitchell, Chelsea Finn</div>

    <span class="periodical">
      
      <em>EMNLP 2024</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://openreview.net/forum?id=BgfGqNpoMi" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Calibrating post-trained models via tokenwise temperature scaling.</strong> </p>
      <p>
    The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration—how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.
    </p>
    </span>
    
  </div>
</div>
</li>
</ol>

  <div class="year">2023</div>
  <ol class="bibliography">
<li>
<div class="row publication-row" id="chen2023cosmos">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICLR-W</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Confidence-Based Model Selection: When to Take Shortcuts for Subpopulation Shifts</b></span>

    <!-- Author -->
    <div class="author">
      

      Annie S. Chen, <strong>Yoonho Lee</strong>, Amrith Setlur, Sergey Levine, Chelsea Finn</div>

    <span class="periodical">
      
      <em>NeurIPS 2023 Workshop on Distribution Shifts</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2306.11120" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Per-sample model selection based for selectively choosing between robust and non-robust models.</strong> </p>
      <p>
    Effective machine learning models learn both robust features that directly determine the outcome of interest (e.g., an object with wheels is more likely to be a car), and shortcut features (e.g., an object on a road is more likely to be a car). The latter can be a source of error under distributional shift, when the correlations change at test-time. The prevailing sentiment in the robustness literature is to avoid such correlative shortcut features and learn robust predictors. However, while robust predictors perform better on worst-case distributional shifts, they often sacrifice accuracy on majority subpopulations. In this paper, we argue that shortcut features should not be entirely discarded. Instead, if we can identify the subpopulation to which an input belongs, we can adaptively choose among models with different strengths to achieve high performance on both majority and minority subpopulations. We propose COnfidence-baSed MOdel Selection (CosMoS), where we observe that model confidence can effectively guide model selection. Notably, CosMoS does not require any target labels or group annotations, either of which may be difficult to obtain or unavailable. We evaluate CosMoS on four datasets with spurious correlations, each with multiple test sets with varying levels of data distribution shift. We find that CosMoS achieves 2-5% lower average regret across all subpopulations, compared to using only robust predictors or other model aggregation methods.
    </p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="mitchell2023detectgpt">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICML</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</b></span>

    <!-- Author -->
    <div class="author">
      

      Eric Mitchell, <strong>Yoonho Lee</strong>, Alexander Khazatsky, Christopher D Manning, Chelsea Finn</div>

    <span class="periodical">
      
      <em>ICML 2023 (<b>Oral Presentation, top 3%</b>)</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2301.11305" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/eric-mitchell/detect-gpt" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: We develop a method that can detect if a passage is generated by a particular language model. Our method is based on the hypothesis that a passage is likely model-generated if it is near a local maximum in the model’s predictive probability space.
</strong> </p>
      <p>
    The fluency and factual knowledge of large language models (LLMs) heightens the need for corresponding systems to detect whether a piece of text is machine-written. For example, students may use LLMs to complete written assignments, leaving instructors unable to accurately assess student learning. In this paper, we first demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model’s log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g, T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT.
    </p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="lee2022surgical">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICLR</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Surgical Fine-Tuning Improves Adaptation to Distribution Shifts</b></span>

    <!-- Author -->
    <div class="author">
      

      <strong>Yoonho Lee*</strong>, Annie S. Chen*, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, Chelsea Finn</div>

    <span class="periodical">
      
      <em>

    ICLR 2023 <br>
    NeurIPS 2022 Workshops: DistShift, ICBINB
    </em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2210.11466" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/anniesch/surgical-finetuning" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: The best layer to fine-tune reflects the nature of the distribution shift.</strong> </p>
      <p>A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shift.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="lee2022divdis">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICLR</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement</b></span>

    <!-- Author -->
    <div class="author">
      

      <strong>Yoonho Lee</strong>, Huaxiu Yao, Chelsea Finn</div>

    <span class="periodical">
      
      <em>ICLR 2023 <br>
  ICML workshops: PODS, SCIS
  </em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2202.03418" target="_blank" rel="external nofollow noopener">paper</a>]     [<a class="publink" href="https://sites.google.com/view/diversify-and-disambiguate" target="_blank" rel="external nofollow noopener">website</a>]      [<a class="publink" href="https://github.com/yoonholee/DivDis" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Given underspecified data, (1) find a diverse set of solutions and (2) choose the best one.</strong> </p>
      <p>
    Many datasets are underspecified: there exist multiple equally viable solutions to a given task. Underspecification can be problematic for methods that learn a single hypothesis because different functions that achieve low training loss can focus on different predictive features and thus produce widely varying predictions on out-of-distribution data. We propose DivDis, a simple two-stage framework that first learns a diverse collection of hypotheses for a task by leveraging unlabeled data from the test distribution. We then disambiguate by selecting one of the discovered hypotheses using minimal additional supervision, in the form of additional labels or inspection of function visualization. We demonstrate the ability of DivDis to find hypotheses that use robust features in image classification and natural language processing problems with underspecification.
    </p>
    </span>
    
  </div>
</div>
</li>
</ol>

  <div class="year">2022</div>
  <ol class="bibliography">
<li>
<div class="row publication-row" id="lee2022ksf">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS-W</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Relaxing the Kolmogorov Structure Function for Realistic Computational Constraints</b></span>

    <!-- Author -->
    <div class="author">
      

      <strong>Yoonho Lee</strong>, Chelsea Finn, Stefano Ermon</div>

    <span class="periodical">
      
      <em>NeurIPS 2022 Workshop on Information-Theoretic Principles in Cognitive Systems</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://openreview.net/forum?id=I1bW14EvUF7" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: An efficient relaxation of the Kolmogorov Structure Function that can leverage neural networks.</strong> </p>
      <p>The degree to which a task is learnable given different computational constraints shows the amount of usable information at different scales. An instantiation of this idea is the Kolmogorov Structure Function (KSF), which shows how the fit of an optimal k-bit description of a given string improves for increasing values of k. While conceptually appealing, computing the KSF is infeasible in practice due to the exponentially large search space of all descriptions of a given length, in addition to the unbounded time complexity. This paper proposes the Constrained Structure Function (CSF), a generalization of the KSF that can be computed efficiently by taking into account realistic computational constraints. In addition to being feasible to compute, the CSF of a dataset can be expressed as the sum of datapoint-wise functions which reflect the degree to which each datapoint is typical in the context of the dataset. Empirically, we demonstrate that the CSF can be used for detecting individual datapoints with characteristics such as being easy, mislabeled, or belonging to a hidden subgroup.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="yao2022wildtime">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time</b></span>

    <!-- Author -->
    <div class="author">
      

      Huaxiu Yao*, Caroline Choi*, Bochuan Cao, <strong>Yoonho Lee</strong>, Pang Wei Koh, Chelsea Finn</div>

    <span class="periodical">
      
      <em>NeurIPS 2022 Datasets &amp; Benchmarks Track <br>
  ICML 2022 Shift Happens Workshop</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2211.14238" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/huaxiuyao/Wild-Time" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: A benchmark of distribution shifts over time.</strong> </p>
      <p>Distribution shifts occur when the test distribution differs from the training distribution, and can considerably degrade performance of machine learning models deployed in the real world. While recent works have studied robustness to distribution shifts, distribution shifts arising from the passage of time have the additional structure of timestamp metadata. Real-world examples of such shifts are underexplored, and it is unclear whether existing models can leverage trends in past distribution shifts to reliably extrapolate into the future. To address this gap, we curate Wild-Time, a benchmark of 7 datasets that reflect temporal distribution shifts arising in a variety of real-world applications. On these datasets, we systematically benchmark 9 approaches with various inductive biases. Our experiments demonstrate that existing methods are limited in tackling temporal distribution shift: across all settings, we observe an average performance drop of 21% from in-distribution to out-of-distribution data.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="kim2022">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>On Divergence Measures for Bayesian Pseudocoresets</b></span>

    <!-- Author -->
    <div class="author">
      

      Balhae Kim, Jungwon Choi, Seanie Lee, <strong>Yoonho Lee</strong>, Jung-Woo Ha, Juho Lee</div>

    <span class="periodical">
      
      <em>NeurIPS 2022</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2210.06205" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/balhaekim/BPC-Divergences" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: An exploration of the choice of divergence for learning a Bayesian pseudocoreset.</strong> </p>
      <p>A Bayesian pseudocoreset is a small synthetic dataset for which the posterior over parameters approximates that of the original dataset. While promising, the scalability of Bayesian pseudocoresets is not yet validated in large-scale problems such as image classification with deep neural networks. On the other hand, dataset distillation methods similarly construct a small dataset such that the optimization with the synthetic dataset converges to a solution similar to optimization with full data. Although dataset distillation has been empirically verified in large-scale settings, the framework is restricted to point estimates, and their adaptation to Bayesian inference has not been explored. This paper casts two representative dataset distillation algorithms as approximations to methods for constructing pseudocoresets by minimizing specific divergence measures: reverse KL divergence and Wasserstein distance. Furthermore, we provide a unifying view of such divergence measures in Bayesian pseudocoreset construction. Finally, we propose a novel Bayesian pseudocoreset algorithm based on minimizing forward KL divergence. Our empirical results demonstrate that the pseudocoresets constructed from these methods reflect the true posterior even in large-scale Bayesian inference problems.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="lee2019discrete">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">Entropy</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Discrete Infomax Codes for Supervised Representation Learning</b></span>

    <!-- Author -->
    <div class="author">
      

      <strong>Yoonho Lee</strong>, Wonjae Kim, Wonpyo Park, Seungjin Choi</div>

    <span class="periodical">
      
      <em>Entropy Special Issue "Theory and Applications of Information Processing Algorithms"</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://www.mdpi.com/1099-4300/24/4/501" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Regularizing few-shot classification using compact discrete codes.</strong> </p>
      <p>Learning compact discrete representations of data is a key task on its own or for facilitating subsequent processing of data. In this paper we present a model that produces Discrete InfoMax COdes (DIMCO); we learn a probabilistic encoder that yields k-way d-dimensional codes associated with input data. Our model’s learning objective is to maximize the mutual information between codes and labels with a regularization, which enforces entries of a codeword to be as independent as possible. We show that the infomax principle also justiﬁes previous loss functions (e.g., cross-entropy) as its special cases. Our analysis also shows that using shorter codes, as DIMCO does, reduces overﬁtting in the conext of few-shot classiﬁcation. Through experiments in various domains, we observe this implicit meta-regularization effect of DIMCO. Furthermore, we show that the codes learned by DIMCO are efﬁcient in terms of both memory and retrieval time compared to previous methods.</p>
    </span>
    
  </div>
</div>
</li>
</ol>

  <div class="year">2021</div>
  <ol class="bibliography">
<li>
<div class="row publication-row" id="nam2021diversity">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Diversity Matters When Learning From Ensembles</b></span>

    <!-- Author -->
    <div class="author">
      

      Giung Nam*, Jongmin Yoon*, <strong>Yoonho Lee</strong>, Juho Lee</div>

    <span class="periodical">
      
      <em>NeurIPS 2021</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2110.14149" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/cs-giung/giung2/tree/main/projects/Diversity-Matters" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: To distill from deep ensembles, use inputs that ensemble members disagree on.</strong> </p>
      <p>
    Deep ensembles excel in large-scale image classification tasks both in terms of prediction accuracy and calibration. Despite being simple to train, the computation and memory cost of deep ensembles limits their practicability. While some recent works propose to distill an ensemble model into a single model to reduce such costs, there is still a performance gap between the ensemble and distilled models. We propose a simple approach for reducing this gap, i.e., making the distilled performance close to the full ensemble. Our key assumption is that a distilled model should absorb as much function diversity inside the ensemble as possible. We first empirically show that the typical distillation procedure does not effectively transfer such diversity, especially for complex models that achieve near-zero training error. To fix this, we propose an augmentation-based distillation strategy that reveals diversity by seeking inputs for which ensemble member outputs disagree. We empirically show that a model distilled with such augmented samples indeed exhibits enhanced diversity, leading to improved performance.
    </p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="pakman2020ACP">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICML-W</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Amortized Probabilistic Detection of Communities in Graphs</b></span>

    <!-- Author -->
    <div class="author">
      

      Yueqi Wang*, <strong>Yoonho Lee*</strong>, Pallab Basu, Juho Lee, Yee Whye Teh, Liam Paninski, Ari Pakman</div>

    <span class="periodical">
      
      <em>ICML 2024 SPIGM workshop</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2010.15727" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/aripakman/attentive_clustering_processes" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: An attention-based method for probabilistically detecting communities within graphs.</strong> </p>
      <p>Learning community structures in graphs has broad applications across scientific domains. While graph neural networks (GNNs) have been successful in encoding graph structures, existing GNN-based methods for community detection are limited by requiring knowledge of the number of communities in advance, in addition to lacking a proper probabilistic formulation to handle uncertainty. We propose a simple framework for amortized community detection, which addresses both of these issues by combining the expressive power of GNNs with recent methods for amortized clustering. Our models consist of a graph representation backbone that extracts structural information and an amortized clustering network that naturally handles variable numbers of clusters. Both components combine into well-defined models of the posterior distribution of graph communities and are jointly optimized given labeled graphs. At inference time, the models yield parallel samples from the posterior of community labels, quantifying uncertainty in a principled way. We evaluate several models from our framework on synthetic and real datasets and demonstrate superior performance to previous methods. As a separate contribution, we extend recent amortized probabilistic clustering architectures by adding attention modules, which yield further improvements on community detection tasks.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="seo2021ckd">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">UAI</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>On the Distribution of Penultimate Activations of Classification Networks</b></span>

    <!-- Author -->
    <div class="author">
      

      Minkyo Seo*, <strong>Yoonho Lee*</strong>, Suha Kwak</div>

    <span class="periodical">
      
      <em>UAI 2021</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2107.01900" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Final FC layer weights contain information about class relations.</strong> </p>
      <p>
    This paper studies the probability distributions of penultimate activations of classification networks. Specifically, we show that, when a classification network is trained with the cross-entropy loss, its final classification layer forms a Generative-Discriminative pair with a generative classifier based on a specific distribution of penultimate activations. More importantly, the distribution is parameterized by the weights of the final fully-connected layer, and can be considered as a generative model that synthesizes the penultimate activations without feeding input data. We empirically demonstrate that this generative model enables stable knowledge distillation in the presence of domain shift, and can also transfer knowledge from a classifier to variational autoencoders and generative adversarial networks for class-conditional image generation.
    </p>
    </span>
    
  </div>
</div>
</li>
</ol>

  <div class="year">2020</div>
  <ol class="bibliography">
<li>
<div class="row publication-row" id="lee2020bootstrapping">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Bootstrapping Neural Processes</b></span>

    <!-- Author -->
    <div class="author">
      

      Juho Lee*, <strong>Yoonho Lee*</strong>, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, Yee Whye Teh</div>

    <span class="periodical">
      
      <em>NeurIPS 2020</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2008.02956" target="_blank" rel="external nofollow noopener">paper</a>]    [<a class="publink" href="https://crossminds.ai/video/5fce48f0d78d8048ceb6e3be/" target="_blank" rel="external nofollow noopener">video</a>]       [<a class="publink" href="https://github.com/juho-lee/bnp" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Improved uncertainty estimates in Neural Processes using bootstrapping.</strong> </p>
      <p>Unlike in the traditional statistical modeling for which a user typically hand-specify a prior, Neural Processes (NPs) implicitly define a broad class of stochastic processes with neural networks. Given a data stream, NP learns a stochastic process that best describes the data. While this "data-driven" way of learning stochastic processes has proven to handle various types of data, NPs still rely on an assumption that uncertainty in stochastic processes is modeled by a single latent variable, which potentially limits the flexibility. To this end, we propose the Boostrapping Neural Process (BNP), a novel extension of the NP family using the bootstrap. The bootstrap is a classical data-driven technique for estimating uncertainty, which allows BNP to learn the stochasticity in NPs without assuming a particular form. We demonstrate the efficacy of BNP on various types of data and its robustness in the presence of model-data mismatch.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="lee2020neural">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Neural Complexity Measures</b></span>

    <!-- Author -->
    <div class="author">
      

      <strong>Yoonho Lee</strong>, Juho Lee, Sung Ju Hwang, Eunho Yang, Seungjin Choi</div>

    <span class="periodical">
      
      <em>NeurIPS 2020</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/2008.02953" target="_blank" rel="external nofollow noopener">paper</a>]    [<a class="publink" href="https://crossminds.ai/video/5fce4f33d78d8048ceb6e3c5/" target="_blank" rel="external nofollow noopener">video</a>]       [<a class="publink" href="https://github.com/yoonholee/neural-complexity" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: A meta-learning framework for predicting generalization.</strong> </p>
      <p>While various complexity measures for deep neural networks exist, specifying an appropriate measure capable of predicting and explaining generalization in deep networks has proven challenging. We propose Neural Complexity (NC), a meta-learning framework for predicting generalization. Our model learns a scalar complexity measure through interactions with many heterogeneous tasks in a data-driven way. The trained NC model can be added to the standard training loss to regularize any task learner in a standard supervised learning scenario. We contrast NC’s approach against existing manually-designed complexity measures and other meta-learning models, and we validate NC’s performance on multiple regression and classification tasks.</p>
    </span>
    
  </div>
</div>
</li>
</ol>

  <div class="year">2019</div>
  <ol class="bibliography">
<li>
<div class="row publication-row" id="lee2019deep">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS-W</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Deep Amortized Clustering</b></span>

    <!-- Author -->
    <div class="author">
      

      Juho Lee, <strong>Yoonho Lee</strong>, Yee Whye Teh</div>

    <span class="periodical">
      
      <em>NeurIPS 2019 Sets and Parts Workshop (<b>oral</b>)</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/1909.13433" target="_blank" rel="external nofollow noopener">paper</a>]         
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Learning to cluster by identifying one cluster at a time.</strong> </p>
      <p>We propose Deep Amortized Clustering (DAC), a framework in which a neural network learns to cluster datasets efficiently using a few forward passes through a deep neural network. DAC implicitly learns what makes a cluster, how to group data points into clusters, and how to count the number of clusters in datasets. DAC is meta-learned in a data-driven way, using only clustered datasets and their partitions. This framework differs from traditional clustering algorithms, which usually require user-specified prior knowledge about the shape or structure of clusters. We empirically show on both synthetic and image data that DAC can efficiently and accurately cluster novel datasets.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="kim2019learning">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">NeurIPS</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning</b></span>

    <!-- Author -->
    <div class="author">
      

      Wonjae Kim, <strong>Yoonho Lee</strong>
</div>

    <span class="periodical">
      
      <em>NeurIPS 2019</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="https://arxiv.org/abs/1905.11666" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/kakao/DAFT" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Smooth and interpretable attention using Neural ODEs.</strong> </p>
      <p>Without relevant human priors, neural networks may learn uninterpretable features. We propose Dynamics of Attention for Focus Transition (DAFT) as a human prior for machine reasoning. DAFT is a novel method that regularizes attention-based reasoning by modelling it as a continuous dynamical system using neural ordinary differential equations. As a proof of concept, we augment a state-of-the-art visual reasoning model with DAFT. Our experiments reveal that applying DAFT yields similar performance to the original model while using fewer reasoning steps, showing that it implicitly learns to skip unnecessary steps. We also propose a new metric, Total Length of Transition (TLT), which represents the effective reasoning step size by quantifying how much a given model’s focus drifts while reasoning about a question. We show that adding DAFT results in lower TLT, demonstrating that our method indeed obeys the human prior towards shorter reasoning paths in addition to producing more interpretable attention maps.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row publication-row" id="pmlr-v97-lee19d">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICML</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks</b></span>

    <!-- Author -->
    <div class="author">
      

      Juho Lee, <strong>Yoonho Lee</strong>, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, Yee Whye Teh</div>

    <span class="periodical">
      
      <em>ICML 2019</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="http://proceedings.mlr.press/v97/lee19d/lee19d.pdf" target="_blank" rel="external nofollow noopener">paper</a>]         [<a class="publink" href="https://github.com/juho-lee/set_transformer" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Self-attention for sets using inducing points. O(N) feedforward complexity.</strong> </p>
      <p>Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.</p>
    </span>
    
  </div>
</div>
</li>
</ol>

  <div class="year">2018</div>
  <ol class="bibliography"><li>
<div class="row publication-row" id="lee2018gradient">
  <div class="col-sm-2 abbr">
     
    <abbr class="my-badge">ICML</abbr>
     
  </div>

  <div class="col-sm-10">
    

    <!-- Title -->
    <span class="title"><b>Gradient-based Meta-learning with Learned Layerwise Metric and Subspace</b></span>

    <!-- Author -->
    <div class="author">
      

      <strong>Yoonho Lee</strong>, Seungjin Choi</div>

    <span class="periodical">
      
      <em>ICML 2018</em>
      
    </span>
    

    <span class="links">
      
      [<a class="abstract publink" href="#">abstract</a>]
      
        [<a class="publink" href="http://proceedings.mlr.press/v80/lee18a/lee18a.pdf" target="_blank" rel="external nofollow noopener">paper</a>]    [<a class="publink" href="https://youtu.be/skcErc5DBYM?t=2904" target="_blank" rel="external nofollow noopener">video</a>]       [<a class="publink" href="https://github.com/yoonholee/MT-net" target="_blank" rel="external nofollow noopener">code</a>]  
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> <strong>TLDR: Improving MAML by fixing some weights during task adaptation.</strong> </p>
      <p>
    Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks.
    While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing.
    Our primary contribution is the MT-net, which enables the meta-learner to learn on each layer’s activation space a subspace that the task-specific learner performs gradient descent on.
    Additionally, a task-specific learner of an MT-net performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity.
    We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner’s adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods.
    Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.</p>
    </span>
    
  </div>
</div>
</li></ol>


</div>

  </article>
</div>
  </div>

  <!-- Footer -->    <!-- <footer class="sticky-bottom mt-5">
      <div class="container">
      </div>
    </footer> -->

<div id="footer" class="navbar-brand social">
  <div class="contact-icons">
  <span class="contact-icon text-center">
    <!-- <h4>Contact Info</h4> -->
    <h4>Yoonho Lee</h4>
    <a href="mailto:%79%6F%6F%6E%68%6F@%73%74%61%6E%66%6F%72%64.%65%64%75" name="email%20link">
      📩 Email</a>
    <span class="contact-separator"> · </span>
    <a rel="noopener" href="https://scholar.google.com/citations?user=BAAZ_ysAAAAJ" target="_blank" title="Google Scholar">
      📖 Google Scholar</a>
    <span class="contact-separator"> · </span>
    <a rel="noopener" href="https://github.com/yoonholee" target="_blank" title="GitHub">
      🐙 Github</a>
    <span class="contact-separator"> · </span>
    <a rel="noopener" href="https://twitter.com/yoonholeee" target="_blank" title="Twitter">
      🐦 Twitter</a>
  <!-- 
    <a href="mailto:%79%6F%6F%6E%68%6F@%73%74%61%6E%66%6F%72%64.%65%64%75" name="email link"><i class="fas fa-envelope"></i></a>
    <a rel="noopener" href="https://scholar.google.com/citations?user=BAAZ_ysAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
    <a rel="noopener" href="https://github.com/yoonholee" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
    
    <a rel="noopener" href="https://twitter.com/yoonholeee" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a> -->
  </span>
  
  </div>

  <div class="contact-note">
    
  </div>
</div>

  <!-- JavaScripts -->
  <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="{" js>"sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4="}" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="{" js>"sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI="}" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
  
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="{" js>"sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc="}" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

  <!-- Load ToC JS for blog posts -->
  

  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-56028200-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-56028200-1');
  </script>
  

<!-- Progress bar that shows reading position while scrolling -->
<script type="text/javascript">
  const progressBar = $("#progress");

  // Initialize after DOM loads to handle image resizing
  document.addEventListener('DOMContentLoaded', () => {
    setTimeout(progressBarSetup, 100);
  });

  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", onScroll);
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", onScroll);
      $(window).on("resize", resizeProgressBar);
    }
  }

  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }

  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }

  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }

  function onScroll() {
    window.requestAnimationFrame(() => {
      if ("max" in document.createElement("progress")) {
        progressBar.attr({ value: getCurrentScrollPosition() });
      } else {
        resizeProgressBar();
      }
    });
  }
</script>

  <script src="/assets/js/performance.js" defer></script>

</body>

</html>
